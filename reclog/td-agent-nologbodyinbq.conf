####
## Source descriptions:
##

## built-in TCP input
## @see http://docs.fluentd.org/articles/in_forward
<source>
  @type forward
</source>

## built-in UNIX socket input
#<source>
#  @type unix
#</source>

# HTTP input
# POST http://localhost:8888/<tag>?json=<json>
# POST http://localhost:8888/td.myapp.login?json={"user"%3A"me"}
# @see http://docs.fluentd.org/articles/in_http
<source>
  @type http
  port 8888
</source>

## live debugging agent
<source>
  @type debug_agent
  bind 127.0.0.1
  port 24230
</source>

## File input
## read apache logs continuously and tags td.apache.access
<source>
  @type tail
#  format none
  format /^(?<logdt>[^|]*)\|(?<loglevel>[^|]*)\|(?<logbody>.*)$/

  #-- recommendation log
  path /var/log/apir_recomd_log_file.log
  pos_file /var/log/td-agent/apir_recomd.log.pos

  tag apir.*
</source>



####
## Filters
##

# unfold input json,i.e. $logbody, into hash record
<filter apir.**>
  @type parser
  format json
  key_name logbody
  reserve_data true
</filter>

<filter apir.**>
  @type record_transformer

  # new an empty hash record
  renew_record true

  # add fields(key) to the new record
  keep_keys logdt,recomd_id,code_name,rec_pos,rec_type,rec_code,ven_guid,ven_session,logbody
</filter>



####
## Output descriptions:
##

## File output
## match tag=local.** and write to file
#<match local.**>
<match apir.**>
  type copy_ex

  ###
  ##  Fluentd output plugin to load/insert data into Google Storage.
  ##  @see https://github.com/kaizenplatform/fluent-plugin-bigquery
  ##

#2019/07/22 test
#  <store ignore_error>
#    @type file
#    path /tmp/apir-td-agent-access
#  </store>
  <store ignore_error>                                    
    type google_cloud_storage_out                         
    service_account_json_key_path /etc/td-agent/PChomeEC-venraas-3cebdedcbeef.json
    project_id pchomeecvenraas                            
    bucket_id pchomeecvenraas-td-agent                    
    path log/apir/%Y%m%d/apir_recomd_log_file.${hostname}.%Y%m%d_%H%M.${unique}.log
    unique_strategy increment                             


    buffer_type file                                      
    buffer_path /var/log/td-agent/buffer/gcs/apir         
    buffer_chunk_limit 50m                                
    flush_interval 30m                                    
  </store>




  ###
  ##  Fluentd output plugin to load/insert data into Google BigQuery.
  ##  @see https://github.com/kaizenplatform/fluent-plugin-bigquery
  ##
  <store ignore_error>
    @type relabel
    @label @NOTIFICATION
  </store>
</match>

<label @NOTIFICATION>
  <filter apir.**>
    @type record_transformer

    # new an empty hash record
    renew_record true

    # add fields(key) to the new record
    keep_keys logdt,recomd_id,code_name,rec_pos,rec_type,rec_code,ven_guid,ven_session
  </filter>

#2019/07/22 test
#  <match apir.**>
#    @type file
#    path /tmp/apir-td-agent-access-nologbody
#  </match>

<match apir.**>
@type copy 
  <store ignore_error>                                       
    @type bigquery                                           
    method insert    # default                                                                
    #-- Buffer Plugin for high rate inserts over streaming traffic.                           
    #  see http://docs.fluentd.org/v0.12/articles/buffer-plugin-overview                     
     buffer_type file
     buffer_path /var/log/td-agent/buffer/bq/apir             
     buffer_chunk_limit 8m                                    
     buffer_queue_limit 64                                    
     flush_interval 1m                                        
     flush_thread_count 5                                     
     #-- BigQuery Authentication
     auth_method private_key   # default                                       
     email raas-system@pchomeecvenraas.iam.gserviceaccount.com
     private_key_path /etc/td-agent/PChomeEC-venraas-80b393060ab7.p12
     #private_key_passphrase notasecret # default                                              
     project pchomeecvenraas                                  
     dataset glog                                             
     table apir_tdagent_%Y%m%d                                
     #table tmp_apir_tdagent_%Y%m%d                                                            
     auto_create_table true                                   
     schema [                                                 
       {"name": "logdt", "type": "STRING"},
       {"name": "recomd_id", "type": "STRING"},               
       {"name": "code_name", "type": "STRING"},               
       {"name": "rec_pos", "type": "STRING"},                 
       {"name": "rec_type", "type": "STRING"},                
       {"name": "rec_code", "type": "STRING"},                
       {"name": "ven_guid", "type": "STRING"},                
       {"name": "ven_session", "type": "STRING"}             
    #  {"name": "logbody", "type": "STRING"}                  
     ]                                                        
    </store>
</match>

</label>

## match tag=debug.** and dump to console
<match debug.**>
  @type stdout
</match>


## Forwarding
## match tag=system.** and forward to another td-agent server
#<match system.**>
#  @type forward
#  host 192.168.0.11
#  # secondary host is optional
#  <secondary>
#    host 192.168.0.12
#  </secondary>
#</match>

## Multiple output
## match tag=td.*.* and output to Treasure Data AND file
#<match td.*.*>
#  @type copy
#  <store>
#    @type tdlog
#    apikey API_KEY
#    auto_create_table
#    buffer_type file
#    buffer_path /var/log/td-agent/buffer/td
#  </store>
#  <store>
#    @type file
#    path /var/log/td-agent/td-%Y-%m-%d/%H.log
#  </store>
#</match>
